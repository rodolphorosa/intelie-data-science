{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Autoencoder for Collaborative Filtering.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodolphorosa/intelie-data-science/blob/master/Autoencoder_for_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY7rVCzZKZsq",
        "colab_type": "text"
      },
      "source": [
        "## Autoencoder for Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZqNMh4xKZsu",
        "colab_type": "text"
      },
      "source": [
        "In this work, the aim is to design a user-based autoencoder which can predict users' ratings based on their historical data. Each user is represented by a partially observed (that is, sparse) vector whose values correspond to their previous ratings. So, the three layer autoencoder takes a user vector as input, projects it to a low-dimensional space (hidden layer) and then reconstructs it in the output layer, so that the missing values are filled. The model proposed is based on the works of Suvash Sedhain et al. (``AutoRec: Autoencoders Meet Collaborative Filtering``) and Kuchaiev & Ginsburg (``Training Deep AutoEncoders for Collaborative Filtering``). \n",
        "\n",
        "ReLu (Rectified Linear Unit) was chosen as activation (transfer) function of the input layer. Although sigmoid and hyperbolic tangent are widely used as activation function in many machine learning problemns and can , they could not certainly work so well in this scenario. First of all, the output they generate are bounded by a very tiny interval (\\[0, 1\\] for sigmoid and \\[-1, 1\\]) for hyperbolic tangent. This is bad firstly because the possibility of learning zero ratings, which is wanted to be avoided, and for the fact that the tangent function may generate negative values, which is even worse. Besides, both functions are prone to the vanishing gradient problem, which prevents the update of the weights of the learning algorithm. Despite ReLu turns negative input into zero, this functions does not have superior limit, which is more adequate to this problem. Forthermore, ReLu overcomes the vanishing learning problem, then the algorithm learns faster. The formula of ReLu is the following:\n",
        "\n",
        "$$ReLu=max\\{0, x\\}$$\n",
        "\n",
        "Due to the nature of the given problem, a linear function was chosen as transfer function between the hidden and the output layer. Once our problem is not probabilistic, sigmoid or softmax are not good choices once they describe, respectively, logistic and probabilistic distribution. Besides, they are both bounded.\n",
        "\n",
        "Mean absolute error as chosen as loss function. This way, the proposed model tries to minimize the average difference between the actual ratings and the predicted ones. Once, predicting zeros is not wanted, a custom version of this function is used. In this version, only the difference between observed ratings and their corresponding outputs is calculated. Thus, it is garanteed that missing values will be learnt. The formula is the following: \n",
        "\n",
        "$$MAE=\\sum_{i=0}^{n}\\frac{|r_{i} - y_{i}|}{n}$$\n",
        "\n",
        "The number of hidden unit were chosen based on the results achieve by the previosu works. This way, the size of hidden layer of this model is of 128, which is good to avoid overfitting during the training. The weights of the neural network are learnt by stochastic gradient descent algorithm, learning rate of 0.001 and momentum of 0.9.\n",
        "\n",
        "To evaluate the performance of the model, k-fold cross-validation was used, with $ k = 5 $. The rating matrix was divided into five disjoined sets of training and test data. 80% of the input was used as training, while 20% was used for testing. The number of epochs in each experiment is 100. \n",
        "\n",
        "The implementation of the autoencoder was done by using Python's machine learning library Keras. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSnRDnDLKZsv",
        "colab_type": "text"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTcybB6JKZsx",
        "colab_type": "text"
      },
      "source": [
        "Keras was used to implement the core of this approach, which is autoencoder itself. Scikit Learn library provides the tools for the cross validation, in this case, K-fold. Finally, Scipy and Numpy were used for data representation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VqP5ZT-5KZsz",
        "colab_type": "code",
        "colab": {},
        "outputId": "021c4a3a-bda4-4355-c991-46183a32a383"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense \n",
        "from keras import backend as K, optimizers \n",
        "from sklearn.model_selection import KFold \n",
        "from scipy import sparse \n",
        "\n",
        "import numpy as np "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXDJIV8LKZs6",
        "colab_type": "text"
      },
      "source": [
        "The following method implements the Mean Squared Error. In this work, its formula has been modified so that only the error between observed ratings and their corresponding output in the reconstructed vector is measured. This prevents the algorithm to learn zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEFvNGw4KZs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae(y_true, y_pred):\n",
        "    return K.mean(K.abs(y_true[y_true > 0] - y_pred[y_true > 0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5janMA81KZtA",
        "colab_type": "text"
      },
      "source": [
        "A rating matrix R is then built with dimension (6040, 3952). Each line comprises a user, whereas each column represents a movie. Once the number of observed ratings is very small compared to the size of the matrix (only 4% of the matrix is filled), it is then converted into a sparse matrix, what significantly reduces the use of memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfzAjHszKZtB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_users = 6040\n",
        "n_movies = 3952\n",
        "\n",
        "R = np.zeros((n_users, n_movies))\n",
        "\n",
        "with open(\"ratings.dat\") as file:\n",
        "    for line in file.readlines():\n",
        "        user_id, movie_id, rating, timestamp = line.split(\"::\")\n",
        "        R[int(user_id) - 1, int(movie_id) - 1] = rating\n",
        "\n",
        "R = sparse.csr_matrix(R)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOcNcEH5KZtG",
        "colab_type": "text"
      },
      "source": [
        "The model is then constructed as a sequence of dense (fully connected) layers. The transfer function from the input to the hidden layer is ReLu, while the hidden layer has a linear function as activaton. The loss function is the aforementioned custom MAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvhgLExqKZtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=3952, activation=\"relu\", use_bias=True))\n",
        "model.add(Dense(3952, activation=\"linear\"))\n",
        "sgd = optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "model.compile(loss=mae, optimizer=sgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwixYRQUKZtN",
        "colab_type": "text"
      },
      "source": [
        "The sparse rating matrix R is then divided into 5 disjoined sets of training and test data. For each pair of training and test sets, the autoencoder learns the ratings from the training set and then the quality of the learning is evaluated with the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNrs3zn8KZtO",
        "colab_type": "code",
        "colab": {},
        "outputId": "bacd2740-aa7b-4a2a-cc1c-e60201d852d9"
      },
      "source": [
        "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "evaluations = []\n",
        "\n",
        "for train, test in kf.split(R):\n",
        "    errors = []\n",
        "    \n",
        "    X_train = R[train]\n",
        "    X_test = R[test]\n",
        "    \n",
        "    model.fit(X_train, X_train, epochs=100, verbose=2)\n",
        "    \n",
        "    evaluations.append(model.evaluate(X_test, X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            " - 2s - loss: 3.5365\n",
            "Epoch 2/100\n",
            " - 2s - loss: 3.0122\n",
            "Epoch 3/100\n",
            " - 2s - loss: 2.1498\n",
            "Epoch 4/100\n",
            " - 2s - loss: 1.8543\n",
            "Epoch 5/100\n",
            " - 2s - loss: 1.7199\n",
            "Epoch 6/100\n",
            " - 2s - loss: 1.6319\n",
            "Epoch 7/100\n",
            " - 2s - loss: 1.5610\n",
            "Epoch 8/100\n",
            " - 2s - loss: 1.5072\n",
            "Epoch 9/100\n",
            " - 2s - loss: 1.4571\n",
            "Epoch 10/100\n",
            " - 2s - loss: 1.4155\n",
            "Epoch 11/100\n",
            " - 2s - loss: 1.3922\n",
            "Epoch 12/100\n",
            " - 2s - loss: 1.3529\n",
            "Epoch 13/100\n",
            " - 2s - loss: 1.3377\n",
            "Epoch 14/100\n",
            " - 2s - loss: 1.3170\n",
            "Epoch 15/100\n",
            " - 2s - loss: 1.2950\n",
            "Epoch 16/100\n",
            " - 2s - loss: 1.2762\n",
            "Epoch 17/100\n",
            " - 2s - loss: 1.2646\n",
            "Epoch 18/100\n",
            " - 2s - loss: 1.2476\n",
            "Epoch 19/100\n",
            " - 2s - loss: 1.2276\n",
            "Epoch 20/100\n",
            " - 2s - loss: 1.2199\n",
            "Epoch 21/100\n",
            " - 2s - loss: 1.2075\n",
            "Epoch 22/100\n",
            " - 2s - loss: 1.1938\n",
            "Epoch 23/100\n",
            " - 2s - loss: 1.1910\n",
            "Epoch 24/100\n",
            " - 2s - loss: 1.1750\n",
            "Epoch 25/100\n",
            " - 2s - loss: 1.1679\n",
            "Epoch 26/100\n",
            " - 2s - loss: 1.1578\n",
            "Epoch 27/100\n",
            " - 2s - loss: 1.1513\n",
            "Epoch 28/100\n",
            " - 2s - loss: 1.1431\n",
            "Epoch 29/100\n",
            " - 2s - loss: 1.1334\n",
            "Epoch 30/100\n",
            " - 2s - loss: 1.1305\n",
            "Epoch 31/100\n",
            " - 2s - loss: 1.1189\n",
            "Epoch 32/100\n",
            " - 2s - loss: 1.1156\n",
            "Epoch 33/100\n",
            " - 2s - loss: 1.1090\n",
            "Epoch 34/100\n",
            " - 2s - loss: 1.1002\n",
            "Epoch 35/100\n",
            " - 2s - loss: 1.0940\n",
            "Epoch 36/100\n",
            " - 2s - loss: 1.0875\n",
            "Epoch 37/100\n",
            " - 2s - loss: 1.0821\n",
            "Epoch 38/100\n",
            " - 2s - loss: 1.0790\n",
            "Epoch 39/100\n",
            " - 2s - loss: 1.0700\n",
            "Epoch 40/100\n",
            " - 2s - loss: 1.0641\n",
            "Epoch 41/100\n",
            " - 2s - loss: 1.0588\n",
            "Epoch 42/100\n",
            " - 2s - loss: 1.0541\n",
            "Epoch 43/100\n",
            " - 2s - loss: 1.0534\n",
            "Epoch 44/100\n",
            " - 2s - loss: 1.0451\n",
            "Epoch 45/100\n",
            " - 2s - loss: 1.0443\n",
            "Epoch 46/100\n",
            " - 2s - loss: 1.0388\n",
            "Epoch 47/100\n",
            " - 2s - loss: 1.0309\n",
            "Epoch 48/100\n",
            " - 2s - loss: 1.0290\n",
            "Epoch 49/100\n",
            " - 2s - loss: 1.0219\n",
            "Epoch 50/100\n",
            " - 2s - loss: 1.0190\n",
            "Epoch 51/100\n",
            " - 2s - loss: 1.0129\n",
            "Epoch 52/100\n",
            " - 2s - loss: 1.0116\n",
            "Epoch 53/100\n",
            " - 2s - loss: 1.0063\n",
            "Epoch 54/100\n",
            " - 2s - loss: 1.0047\n",
            "Epoch 55/100\n",
            " - 2s - loss: 0.9998\n",
            "Epoch 56/100\n",
            " - 2s - loss: 0.9964\n",
            "Epoch 57/100\n",
            " - 2s - loss: 0.9917\n",
            "Epoch 58/100\n",
            " - 2s - loss: 0.9890\n",
            "Epoch 59/100\n",
            " - 2s - loss: 0.9820\n",
            "Epoch 60/100\n",
            " - 2s - loss: 0.9815\n",
            "Epoch 61/100\n",
            " - 2s - loss: 0.9762\n",
            "Epoch 62/100\n",
            " - 2s - loss: 0.9761\n",
            "Epoch 63/100\n",
            " - 2s - loss: 0.9692\n",
            "Epoch 64/100\n",
            " - 2s - loss: 0.9695\n",
            "Epoch 65/100\n",
            " - 2s - loss: 0.9663\n",
            "Epoch 66/100\n",
            " - 2s - loss: 0.9618\n",
            "Epoch 67/100\n",
            " - 2s - loss: 0.9583\n",
            "Epoch 68/100\n",
            " - 2s - loss: 0.9559\n",
            "Epoch 69/100\n",
            " - 2s - loss: 0.9541\n",
            "Epoch 70/100\n",
            " - 2s - loss: 0.9500\n",
            "Epoch 71/100\n",
            " - 2s - loss: 0.9452\n",
            "Epoch 72/100\n",
            " - 2s - loss: 0.9448\n",
            "Epoch 73/100\n",
            " - 2s - loss: 0.9385\n",
            "Epoch 74/100\n",
            " - 2s - loss: 0.9374\n",
            "Epoch 75/100\n",
            " - 2s - loss: 0.9349\n",
            "Epoch 76/100\n",
            " - 2s - loss: 0.9332\n",
            "Epoch 77/100\n",
            " - 2s - loss: 0.9287\n",
            "Epoch 78/100\n",
            " - 2s - loss: 0.9260\n",
            "Epoch 79/100\n",
            " - 2s - loss: 0.9230\n",
            "Epoch 80/100\n",
            " - 2s - loss: 0.9205\n",
            "Epoch 81/100\n",
            " - 2s - loss: 0.9176\n",
            "Epoch 82/100\n",
            " - 2s - loss: 0.9157\n",
            "Epoch 83/100\n",
            " - 2s - loss: 0.9139\n",
            "Epoch 84/100\n",
            " - 2s - loss: 0.9143\n",
            "Epoch 85/100\n",
            " - 2s - loss: 0.9097\n",
            "Epoch 86/100\n",
            " - 2s - loss: 0.9081\n",
            "Epoch 87/100\n",
            " - 2s - loss: 0.9053\n",
            "Epoch 88/100\n",
            " - 2s - loss: 0.9016\n",
            "Epoch 89/100\n",
            " - 2s - loss: 0.9008\n",
            "Epoch 90/100\n",
            " - 2s - loss: 0.8995\n",
            "Epoch 91/100\n",
            " - 2s - loss: 0.8965\n",
            "Epoch 92/100\n",
            " - 2s - loss: 0.8926\n",
            "Epoch 93/100\n",
            " - 2s - loss: 0.8921\n",
            "Epoch 94/100\n",
            " - 2s - loss: 0.8889\n",
            "Epoch 95/100\n",
            " - 2s - loss: 0.8851\n",
            "Epoch 96/100\n",
            " - 2s - loss: 0.8850\n",
            "Epoch 97/100\n",
            " - 2s - loss: 0.8835\n",
            "Epoch 98/100\n",
            " - 2s - loss: 0.8833\n",
            "Epoch 99/100\n",
            " - 2s - loss: 0.8789\n",
            "Epoch 100/100\n",
            " - 2s - loss: 0.8752\n",
            "1208/1208 [==============================] - 0s 181us/step\n",
            "Epoch 1/100\n",
            " - 2s - loss: 0.9233\n",
            "Epoch 2/100\n",
            " - 2s - loss: 0.9150\n",
            "Epoch 3/100\n",
            " - 2s - loss: 0.9118\n",
            "Epoch 4/100\n",
            " - 2s - loss: 0.9099\n",
            "Epoch 5/100\n",
            " - 2s - loss: 0.9049\n",
            "Epoch 6/100\n",
            " - 2s - loss: 0.9032\n",
            "Epoch 7/100\n",
            " - 2s - loss: 0.8988\n",
            "Epoch 8/100\n",
            " - 2s - loss: 0.8971\n",
            "Epoch 9/100\n",
            " - 2s - loss: 0.8953\n",
            "Epoch 10/100\n",
            " - 2s - loss: 0.8919\n",
            "Epoch 11/100\n",
            " - 2s - loss: 0.8890\n",
            "Epoch 12/100\n",
            " - 2s - loss: 0.8864\n",
            "Epoch 13/100\n",
            " - 2s - loss: 0.8847\n",
            "Epoch 14/100\n",
            " - 2s - loss: 0.8834\n",
            "Epoch 15/100\n",
            " - 2s - loss: 0.8796\n",
            "Epoch 16/100\n",
            " - 2s - loss: 0.8778\n",
            "Epoch 17/100\n",
            " - 2s - loss: 0.8725\n",
            "Epoch 18/100\n",
            " - 2s - loss: 0.8690\n",
            "Epoch 19/100\n",
            " - 2s - loss: 0.8678\n",
            "Epoch 20/100\n",
            " - 2s - loss: 0.8686\n",
            "Epoch 21/100\n",
            " - 2s - loss: 0.8639\n",
            "Epoch 22/100\n",
            " - 2s - loss: 0.8600\n",
            "Epoch 23/100\n",
            " - 2s - loss: 0.8628\n",
            "Epoch 24/100\n",
            " - 2s - loss: 0.8604\n",
            "Epoch 25/100\n",
            " - 2s - loss: 0.8566\n",
            "Epoch 26/100\n",
            " - 2s - loss: 0.8559\n",
            "Epoch 27/100\n",
            " - 2s - loss: 0.8529\n",
            "Epoch 28/100\n",
            " - 2s - loss: 0.8510\n",
            "Epoch 29/100\n",
            " - 2s - loss: 0.8488\n",
            "Epoch 30/100\n",
            " - 2s - loss: 0.8497\n",
            "Epoch 31/100\n",
            " - 2s - loss: 0.8448\n",
            "Epoch 32/100\n",
            " - 2s - loss: 0.8435\n",
            "Epoch 33/100\n",
            " - 2s - loss: 0.8445\n",
            "Epoch 34/100\n",
            " - 2s - loss: 0.8392\n",
            "Epoch 35/100\n",
            " - 2s - loss: 0.8377\n",
            "Epoch 36/100\n",
            " - 2s - loss: 0.8362\n",
            "Epoch 37/100\n",
            " - 2s - loss: 0.8328\n",
            "Epoch 38/100\n",
            " - 2s - loss: 0.8350\n",
            "Epoch 39/100\n",
            " - 2s - loss: 0.8296\n",
            "Epoch 40/100\n",
            " - 2s - loss: 0.8300\n",
            "Epoch 41/100\n",
            " - 2s - loss: 0.8277\n",
            "Epoch 42/100\n",
            " - 2s - loss: 0.8248\n",
            "Epoch 43/100\n",
            " - 2s - loss: 0.8269\n",
            "Epoch 44/100\n",
            " - 2s - loss: 0.8229\n",
            "Epoch 45/100\n",
            " - 2s - loss: 0.8227\n",
            "Epoch 46/100\n",
            " - 2s - loss: 0.8215\n",
            "Epoch 47/100\n",
            " - 2s - loss: 0.8179\n",
            "Epoch 48/100\n",
            " - 2s - loss: 0.8187\n",
            "Epoch 49/100\n",
            " - 2s - loss: 0.8180\n",
            "Epoch 50/100\n",
            " - 2s - loss: 0.8150\n",
            "Epoch 51/100\n",
            " - 2s - loss: 0.8127\n",
            "Epoch 52/100\n",
            " - 2s - loss: 0.8108\n",
            "Epoch 53/100\n",
            " - 2s - loss: 0.8110\n",
            "Epoch 54/100\n",
            " - 2s - loss: 0.8089\n",
            "Epoch 55/100\n",
            " - 2s - loss: 0.8086\n",
            "Epoch 56/100\n",
            " - 2s - loss: 0.8061\n",
            "Epoch 57/100\n",
            " - 2s - loss: 0.8052\n",
            "Epoch 58/100\n",
            " - 2s - loss: 0.8041\n",
            "Epoch 59/100\n",
            " - 2s - loss: 0.8016\n",
            "Epoch 60/100\n",
            " - 2s - loss: 0.8003\n",
            "Epoch 61/100\n",
            " - 2s - loss: 0.7990\n",
            "Epoch 62/100\n",
            " - 2s - loss: 0.8001\n",
            "Epoch 63/100\n",
            " - 2s - loss: 0.7966\n",
            "Epoch 64/100\n",
            " - 2s - loss: 0.7976\n",
            "Epoch 65/100\n",
            " - 2s - loss: 0.7967\n",
            "Epoch 66/100\n",
            " - 2s - loss: 0.7932\n",
            "Epoch 67/100\n",
            " - 2s - loss: 0.7929\n",
            "Epoch 68/100\n",
            " - 2s - loss: 0.7930\n",
            "Epoch 69/100\n",
            " - 2s - loss: 0.7901\n",
            "Epoch 70/100\n",
            " - 2s - loss: 0.7888\n",
            "Epoch 71/100\n",
            " - 2s - loss: 0.7881\n",
            "Epoch 72/100\n",
            " - 2s - loss: 0.7850\n",
            "Epoch 73/100\n",
            " - 2s - loss: 0.7873\n",
            "Epoch 74/100\n",
            " - 2s - loss: 0.7822\n",
            "Epoch 75/100\n",
            " - 2s - loss: 0.7827\n",
            "Epoch 76/100\n",
            " - 2s - loss: 0.7831\n",
            "Epoch 77/100\n",
            " - 2s - loss: 0.7804\n",
            "Epoch 78/100\n",
            " - 2s - loss: 0.7778\n",
            "Epoch 79/100\n",
            " - 2s - loss: 0.7791\n",
            "Epoch 80/100\n",
            " - 2s - loss: 0.7774\n",
            "Epoch 81/100\n",
            " - 2s - loss: 0.7761\n",
            "Epoch 82/100\n",
            " - 2s - loss: 0.7742\n",
            "Epoch 83/100\n",
            " - 2s - loss: 0.7735\n",
            "Epoch 84/100\n",
            " - 2s - loss: 0.7723\n",
            "Epoch 85/100\n",
            " - 2s - loss: 0.7712\n",
            "Epoch 86/100\n",
            " - 2s - loss: 0.7703\n",
            "Epoch 87/100\n",
            " - 2s - loss: 0.7701\n",
            "Epoch 88/100\n",
            " - 2s - loss: 0.7704\n",
            "Epoch 89/100\n",
            " - 2s - loss: 0.7662\n",
            "Epoch 90/100\n",
            " - 2s - loss: 0.7673\n",
            "Epoch 91/100\n",
            " - 2s - loss: 0.7648\n",
            "Epoch 92/100\n",
            " - 2s - loss: 0.7644\n",
            "Epoch 93/100\n",
            " - 2s - loss: 0.7620\n",
            "Epoch 94/100\n",
            " - 2s - loss: 0.7629\n",
            "Epoch 95/100\n",
            " - 2s - loss: 0.7614\n",
            "Epoch 96/100\n",
            " - 2s - loss: 0.7598\n",
            "Epoch 97/100\n",
            " - 2s - loss: 0.7598\n",
            "Epoch 98/100\n",
            " - 2s - loss: 0.7573\n",
            "Epoch 99/100\n",
            " - 2s - loss: 0.7576\n",
            "Epoch 100/100\n",
            " - 2s - loss: 0.7565\n",
            "1208/1208 [==============================] - 0s 163us/step\n",
            "Epoch 1/100\n",
            " - 2s - loss: 0.7816\n",
            "Epoch 2/100\n",
            " - 2s - loss: 0.7813\n",
            "Epoch 3/100\n",
            " - 2s - loss: 0.7795\n",
            "Epoch 4/100\n",
            " - 2s - loss: 0.7750\n",
            "Epoch 5/100\n",
            " - 2s - loss: 0.7757\n",
            "Epoch 6/100\n",
            " - 2s - loss: 0.7736\n",
            "Epoch 7/100\n",
            " - 2s - loss: 0.7702\n",
            "Epoch 8/100\n",
            " - 2s - loss: 0.7692\n",
            "Epoch 9/100\n",
            " - 2s - loss: 0.7693\n",
            "Epoch 10/100\n",
            " - 2s - loss: 0.7670\n",
            "Epoch 11/100\n",
            " - 2s - loss: 0.7674\n",
            "Epoch 12/100\n",
            " - 2s - loss: 0.7648\n",
            "Epoch 13/100\n",
            " - 2s - loss: 0.7633\n",
            "Epoch 14/100\n",
            " - 2s - loss: 0.7595\n",
            "Epoch 15/100\n",
            " - 2s - loss: 0.7603\n",
            "Epoch 16/100\n",
            " - 2s - loss: 0.7579\n",
            "Epoch 17/100\n",
            " - 2s - loss: 0.7572\n",
            "Epoch 18/100\n",
            " - 2s - loss: 0.7569\n",
            "Epoch 19/100\n",
            " - 2s - loss: 0.7551\n",
            "Epoch 20/100\n",
            " - 2s - loss: 0.7532\n",
            "Epoch 21/100\n",
            " - 2s - loss: 0.7533\n",
            "Epoch 22/100\n",
            " - 2s - loss: 0.7526\n",
            "Epoch 23/100\n",
            " - 2s - loss: 0.7518\n",
            "Epoch 24/100\n",
            " - 2s - loss: 0.7514\n",
            "Epoch 25/100\n",
            " - 2s - loss: 0.7518\n",
            "Epoch 26/100\n",
            " - 2s - loss: 0.7471\n",
            "Epoch 27/100\n",
            " - 2s - loss: 0.7465\n",
            "Epoch 28/100\n",
            " - 2s - loss: 0.7471\n",
            "Epoch 29/100\n",
            " - 2s - loss: 0.7458\n",
            "Epoch 30/100\n",
            " - 2s - loss: 0.7438\n",
            "Epoch 31/100\n",
            " - 2s - loss: 0.7434\n",
            "Epoch 32/100\n",
            " - 2s - loss: 0.7420\n",
            "Epoch 33/100\n",
            " - 2s - loss: 0.7400\n",
            "Epoch 34/100\n",
            " - 2s - loss: 0.7418\n",
            "Epoch 35/100\n",
            " - 2s - loss: 0.7391\n",
            "Epoch 36/100\n",
            " - 2s - loss: 0.7376\n",
            "Epoch 37/100\n",
            " - 2s - loss: 0.7373\n",
            "Epoch 38/100\n",
            " - 2s - loss: 0.7381\n",
            "Epoch 39/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 2s - loss: 0.7360\n",
            "Epoch 40/100\n",
            " - 2s - loss: 0.7352\n",
            "Epoch 41/100\n",
            " - 2s - loss: 0.7345\n",
            "Epoch 42/100\n",
            " - 2s - loss: 0.7339\n",
            "Epoch 43/100\n",
            " - 2s - loss: 0.7323\n",
            "Epoch 44/100\n",
            " - 2s - loss: 0.7330\n",
            "Epoch 45/100\n",
            " - 2s - loss: 0.7296\n",
            "Epoch 46/100\n",
            " - 2s - loss: 0.7304\n",
            "Epoch 47/100\n",
            " - 2s - loss: 0.7288\n",
            "Epoch 48/100\n",
            " - 2s - loss: 0.7283\n",
            "Epoch 49/100\n",
            " - 2s - loss: 0.7272\n",
            "Epoch 50/100\n",
            " - 2s - loss: 0.7264\n",
            "Epoch 51/100\n",
            " - 2s - loss: 0.7256\n",
            "Epoch 52/100\n",
            " - 2s - loss: 0.7254\n",
            "Epoch 53/100\n",
            " - 2s - loss: 0.7236\n",
            "Epoch 54/100\n",
            " - 2s - loss: 0.7234\n",
            "Epoch 55/100\n",
            " - 2s - loss: 0.7227\n",
            "Epoch 56/100\n",
            " - 2s - loss: 0.7229\n",
            "Epoch 57/100\n",
            " - 2s - loss: 0.7223\n",
            "Epoch 58/100\n",
            " - 2s - loss: 0.7201\n",
            "Epoch 59/100\n",
            " - 2s - loss: 0.7201\n",
            "Epoch 60/100\n",
            " - 2s - loss: 0.7178\n",
            "Epoch 61/100\n",
            " - 2s - loss: 0.7184\n",
            "Epoch 62/100\n",
            " - 2s - loss: 0.7170\n",
            "Epoch 63/100\n",
            " - 2s - loss: 0.7165\n",
            "Epoch 64/100\n",
            " - 2s - loss: 0.7158\n",
            "Epoch 65/100\n",
            " - 2s - loss: 0.7156\n",
            "Epoch 66/100\n",
            " - 2s - loss: 0.7167\n",
            "Epoch 67/100\n",
            " - 2s - loss: 0.7137\n",
            "Epoch 68/100\n",
            " - 2s - loss: 0.7141\n",
            "Epoch 69/100\n",
            " - 2s - loss: 0.7137\n",
            "Epoch 70/100\n",
            " - 2s - loss: 0.7112\n",
            "Epoch 71/100\n",
            " - 2s - loss: 0.7108\n",
            "Epoch 72/100\n",
            " - 2s - loss: 0.7110\n",
            "Epoch 73/100\n",
            " - 2s - loss: 0.7100\n",
            "Epoch 74/100\n",
            " - 2s - loss: 0.7086\n",
            "Epoch 75/100\n",
            " - 2s - loss: 0.7080\n",
            "Epoch 76/100\n",
            " - 2s - loss: 0.7081\n",
            "Epoch 77/100\n",
            " - 2s - loss: 0.7068\n",
            "Epoch 78/100\n",
            " - 2s - loss: 0.7060\n",
            "Epoch 79/100\n",
            " - 2s - loss: 0.7054\n",
            "Epoch 80/100\n",
            " - 2s - loss: 0.7045\n",
            "Epoch 81/100\n",
            " - 2s - loss: 0.7048\n",
            "Epoch 82/100\n",
            " - 2s - loss: 0.7028\n",
            "Epoch 83/100\n",
            " - 2s - loss: 0.7023\n",
            "Epoch 84/100\n",
            " - 2s - loss: 0.7040\n",
            "Epoch 85/100\n",
            " - 2s - loss: 0.7021\n",
            "Epoch 86/100\n",
            " - 2s - loss: 0.7012\n",
            "Epoch 87/100\n",
            " - 2s - loss: 0.7017\n",
            "Epoch 88/100\n",
            " - 2s - loss: 0.6989\n",
            "Epoch 89/100\n",
            " - 2s - loss: 0.7002\n",
            "Epoch 90/100\n",
            " - 2s - loss: 0.6976\n",
            "Epoch 91/100\n",
            " - 2s - loss: 0.6974\n",
            "Epoch 92/100\n",
            " - 2s - loss: 0.6963\n",
            "Epoch 93/100\n",
            " - 2s - loss: 0.6965\n",
            "Epoch 94/100\n",
            " - 2s - loss: 0.6956\n",
            "Epoch 95/100\n",
            " - 2s - loss: 0.6957\n",
            "Epoch 96/100\n",
            " - 2s - loss: 0.6938\n",
            "Epoch 97/100\n",
            " - 2s - loss: 0.6941\n",
            "Epoch 98/100\n",
            " - 2s - loss: 0.6934\n",
            "Epoch 99/100\n",
            " - 2s - loss: 0.6922\n",
            "Epoch 100/100\n",
            " - 2s - loss: 0.6923\n",
            "1208/1208 [==============================] - 0s 164us/step\n",
            "Epoch 1/100\n",
            " - 2s - loss: 0.7147\n",
            "Epoch 2/100\n",
            " - 2s - loss: 0.7127\n",
            "Epoch 3/100\n",
            " - 2s - loss: 0.7114\n",
            "Epoch 4/100\n",
            " - 2s - loss: 0.7101\n",
            "Epoch 5/100\n",
            " - 2s - loss: 0.7087\n",
            "Epoch 6/100\n",
            " - 2s - loss: 0.7079\n",
            "Epoch 7/100\n",
            " - 2s - loss: 0.7061\n",
            "Epoch 8/100\n",
            " - 2s - loss: 0.7059\n",
            "Epoch 9/100\n",
            " - 2s - loss: 0.7050\n",
            "Epoch 10/100\n",
            " - 2s - loss: 0.7044\n",
            "Epoch 11/100\n",
            " - 2s - loss: 0.7040\n",
            "Epoch 12/100\n",
            " - 2s - loss: 0.7029\n",
            "Epoch 13/100\n",
            " - 2s - loss: 0.7045\n",
            "Epoch 14/100\n",
            " - 2s - loss: 0.7011\n",
            "Epoch 15/100\n",
            " - 2s - loss: 0.7004\n",
            "Epoch 16/100\n",
            " - 2s - loss: 0.6993\n",
            "Epoch 17/100\n",
            " - 2s - loss: 0.6977\n",
            "Epoch 18/100\n",
            " - 2s - loss: 0.6974\n",
            "Epoch 19/100\n",
            " - 2s - loss: 0.6979\n",
            "Epoch 20/100\n",
            " - 2s - loss: 0.6969\n",
            "Epoch 21/100\n",
            " - 2s - loss: 0.6956\n",
            "Epoch 22/100\n",
            " - 2s - loss: 0.6953\n",
            "Epoch 23/100\n",
            " - 2s - loss: 0.6947\n",
            "Epoch 24/100\n",
            " - 2s - loss: 0.6947\n",
            "Epoch 25/100\n",
            " - 2s - loss: 0.6938\n",
            "Epoch 26/100\n",
            " - 2s - loss: 0.6921\n",
            "Epoch 27/100\n",
            " - 2s - loss: 0.6923\n",
            "Epoch 28/100\n",
            " - 2s - loss: 0.6910\n",
            "Epoch 29/100\n",
            " - 2s - loss: 0.6910\n",
            "Epoch 30/100\n",
            " - 2s - loss: 0.6891\n",
            "Epoch 31/100\n",
            " - 2s - loss: 0.6888\n",
            "Epoch 32/100\n",
            " - 2s - loss: 0.6888\n",
            "Epoch 33/100\n",
            " - 2s - loss: 0.6882\n",
            "Epoch 34/100\n",
            " - 2s - loss: 0.6872\n",
            "Epoch 35/100\n",
            " - 2s - loss: 0.6868\n",
            "Epoch 36/100\n",
            " - 2s - loss: 0.6869\n",
            "Epoch 37/100\n",
            " - 2s - loss: 0.6864\n",
            "Epoch 38/100\n",
            " - 2s - loss: 0.6852\n",
            "Epoch 39/100\n",
            " - 2s - loss: 0.6852\n",
            "Epoch 40/100\n",
            " - 2s - loss: 0.6837\n",
            "Epoch 41/100\n",
            " - 2s - loss: 0.6837\n",
            "Epoch 42/100\n",
            " - 2s - loss: 0.6841\n",
            "Epoch 43/100\n",
            " - 2s - loss: 0.6823\n",
            "Epoch 44/100\n",
            " - 2s - loss: 0.6813\n",
            "Epoch 45/100\n",
            " - 2s - loss: 0.6796\n",
            "Epoch 46/100\n",
            " - 2s - loss: 0.6798\n",
            "Epoch 47/100\n",
            " - 2s - loss: 0.6802\n",
            "Epoch 48/100\n",
            " - 2s - loss: 0.6794\n",
            "Epoch 49/100\n",
            " - 2s - loss: 0.6796\n",
            "Epoch 50/100\n",
            " - 2s - loss: 0.6780\n",
            "Epoch 51/100\n",
            " - 2s - loss: 0.6775\n",
            "Epoch 52/100\n",
            " - 2s - loss: 0.6761\n",
            "Epoch 53/100\n",
            " - 2s - loss: 0.6776\n",
            "Epoch 54/100\n",
            " - 2s - loss: 0.6752\n",
            "Epoch 55/100\n",
            " - 2s - loss: 0.6747\n",
            "Epoch 56/100\n",
            " - 2s - loss: 0.6737\n",
            "Epoch 57/100\n",
            " - 2s - loss: 0.6738\n",
            "Epoch 58/100\n",
            " - 2s - loss: 0.6751\n",
            "Epoch 59/100\n",
            " - 2s - loss: 0.6720\n",
            "Epoch 60/100\n",
            " - 2s - loss: 0.6727\n",
            "Epoch 61/100\n",
            " - 2s - loss: 0.6714\n",
            "Epoch 62/100\n",
            " - 2s - loss: 0.6713\n",
            "Epoch 63/100\n",
            " - 2s - loss: 0.6700\n",
            "Epoch 64/100\n",
            " - 2s - loss: 0.6712\n",
            "Epoch 65/100\n",
            " - 2s - loss: 0.6691\n",
            "Epoch 66/100\n",
            " - 2s - loss: 0.6702\n",
            "Epoch 67/100\n",
            " - 2s - loss: 0.6687\n",
            "Epoch 68/100\n",
            " - 2s - loss: 0.6672\n",
            "Epoch 69/100\n",
            " - 2s - loss: 0.6678\n",
            "Epoch 70/100\n",
            " - 2s - loss: 0.6662\n",
            "Epoch 71/100\n",
            " - 2s - loss: 0.6670\n",
            "Epoch 72/100\n",
            " - 2s - loss: 0.6662\n",
            "Epoch 73/100\n",
            " - 2s - loss: 0.6659\n",
            "Epoch 74/100\n",
            " - 2s - loss: 0.6645\n",
            "Epoch 75/100\n",
            " - 2s - loss: 0.6647\n",
            "Epoch 76/100\n",
            " - 2s - loss: 0.6634\n",
            "Epoch 77/100\n",
            " - 2s - loss: 0.6642\n",
            "Epoch 78/100\n",
            " - 2s - loss: 0.6636\n",
            "Epoch 79/100\n",
            " - 2s - loss: 0.6621\n",
            "Epoch 80/100\n",
            " - 2s - loss: 0.6616\n",
            "Epoch 81/100\n",
            " - 2s - loss: 0.6625\n",
            "Epoch 82/100\n",
            " - 2s - loss: 0.6621\n",
            "Epoch 83/100\n",
            " - 2s - loss: 0.6606\n",
            "Epoch 84/100\n",
            " - 2s - loss: 0.6597\n",
            "Epoch 85/100\n",
            " - 2s - loss: 0.6611\n",
            "Epoch 86/100\n",
            " - 2s - loss: 0.6598\n",
            "Epoch 87/100\n",
            " - 2s - loss: 0.6596\n",
            "Epoch 88/100\n",
            " - 2s - loss: 0.6582\n",
            "Epoch 89/100\n",
            " - 2s - loss: 0.6577\n",
            "Epoch 90/100\n",
            " - 2s - loss: 0.6581\n",
            "Epoch 91/100\n",
            " - 2s - loss: 0.6568\n",
            "Epoch 92/100\n",
            " - 2s - loss: 0.6573\n",
            "Epoch 93/100\n",
            " - 2s - loss: 0.6558\n",
            "Epoch 94/100\n",
            " - 2s - loss: 0.6552\n",
            "Epoch 95/100\n",
            " - 2s - loss: 0.6550\n",
            "Epoch 96/100\n",
            " - 2s - loss: 0.6555\n",
            "Epoch 97/100\n",
            " - 2s - loss: 0.6546\n",
            "Epoch 98/100\n",
            " - 2s - loss: 0.6529\n",
            "Epoch 99/100\n",
            " - 2s - loss: 0.6532\n",
            "Epoch 100/100\n",
            " - 2s - loss: 0.6525\n",
            "1208/1208 [==============================] - 0s 168us/step\n",
            "Epoch 1/100\n",
            " - 2s - loss: 0.6657\n",
            "Epoch 2/100\n",
            " - 2s - loss: 0.6644\n",
            "Epoch 3/100\n",
            " - 2s - loss: 0.6634\n",
            "Epoch 4/100\n",
            " - 2s - loss: 0.6629\n",
            "Epoch 5/100\n",
            " - 2s - loss: 0.6610\n",
            "Epoch 6/100\n",
            " - 2s - loss: 0.6620\n",
            "Epoch 7/100\n",
            " - 2s - loss: 0.6603\n",
            "Epoch 8/100\n",
            " - 2s - loss: 0.6600\n",
            "Epoch 9/100\n",
            " - 2s - loss: 0.6600\n",
            "Epoch 10/100\n",
            " - 2s - loss: 0.6582\n",
            "Epoch 11/100\n",
            " - 2s - loss: 0.6571\n",
            "Epoch 12/100\n",
            " - 2s - loss: 0.6583\n",
            "Epoch 13/100\n",
            " - 2s - loss: 0.6569\n",
            "Epoch 14/100\n",
            " - 2s - loss: 0.6555\n",
            "Epoch 15/100\n",
            " - 2s - loss: 0.6551\n",
            "Epoch 16/100\n",
            " - 2s - loss: 0.6551\n",
            "Epoch 17/100\n",
            " - 2s - loss: 0.6529\n",
            "Epoch 18/100\n",
            " - 2s - loss: 0.6552\n",
            "Epoch 19/100\n",
            " - 2s - loss: 0.6538\n",
            "Epoch 20/100\n",
            " - 2s - loss: 0.6525\n",
            "Epoch 21/100\n",
            " - 2s - loss: 0.6519\n",
            "Epoch 22/100\n",
            " - 2s - loss: 0.6510\n",
            "Epoch 23/100\n",
            " - 2s - loss: 0.6514\n",
            "Epoch 24/100\n",
            " - 2s - loss: 0.6504\n",
            "Epoch 25/100\n",
            " - 2s - loss: 0.6500\n",
            "Epoch 26/100\n",
            " - 2s - loss: 0.6492\n",
            "Epoch 27/100\n",
            " - 2s - loss: 0.6505\n",
            "Epoch 28/100\n",
            " - 2s - loss: 0.6493\n",
            "Epoch 29/100\n",
            " - 2s - loss: 0.6475\n",
            "Epoch 30/100\n",
            " - 2s - loss: 0.6476\n",
            "Epoch 31/100\n",
            " - 2s - loss: 0.6488\n",
            "Epoch 32/100\n",
            " - 2s - loss: 0.6473\n",
            "Epoch 33/100\n",
            " - 2s - loss: 0.6461\n",
            "Epoch 34/100\n",
            " - 2s - loss: 0.6469\n",
            "Epoch 35/100\n",
            " - 2s - loss: 0.6460\n",
            "Epoch 36/100\n",
            " - 2s - loss: 0.6439\n",
            "Epoch 37/100\n",
            " - 2s - loss: 0.6447\n",
            "Epoch 38/100\n",
            " - 2s - loss: 0.6447\n",
            "Epoch 39/100\n",
            " - 2s - loss: 0.6428\n",
            "Epoch 40/100\n",
            " - 2s - loss: 0.6442\n",
            "Epoch 41/100\n",
            " - 2s - loss: 0.6432\n",
            "Epoch 42/100\n",
            " - 2s - loss: 0.6419\n",
            "Epoch 43/100\n",
            " - 2s - loss: 0.6422\n",
            "Epoch 44/100\n",
            " - 2s - loss: 0.6423\n",
            "Epoch 45/100\n",
            " - 2s - loss: 0.6416\n",
            "Epoch 46/100\n",
            " - 2s - loss: 0.6408\n",
            "Epoch 47/100\n",
            " - 2s - loss: 0.6415\n",
            "Epoch 48/100\n",
            " - 2s - loss: 0.6408\n",
            "Epoch 49/100\n",
            " - 2s - loss: 0.6387\n",
            "Epoch 50/100\n",
            " - 2s - loss: 0.6396\n",
            "Epoch 51/100\n",
            " - 2s - loss: 0.6392\n",
            "Epoch 52/100\n",
            " - 2s - loss: 0.6374\n",
            "Epoch 53/100\n",
            " - 2s - loss: 0.6386\n",
            "Epoch 54/100\n",
            " - 2s - loss: 0.6381\n",
            "Epoch 55/100\n",
            " - 2s - loss: 0.6368\n",
            "Epoch 56/100\n",
            " - 2s - loss: 0.6369\n",
            "Epoch 57/100\n",
            " - 2s - loss: 0.6356\n",
            "Epoch 58/100\n",
            " - 2s - loss: 0.6373\n",
            "Epoch 59/100\n",
            " - 2s - loss: 0.6350\n",
            "Epoch 60/100\n",
            " - 2s - loss: 0.6343\n",
            "Epoch 61/100\n",
            " - 2s - loss: 0.6342\n",
            "Epoch 62/100\n",
            " - 2s - loss: 0.6348\n",
            "Epoch 63/100\n",
            " - 2s - loss: 0.6341\n",
            "Epoch 64/100\n",
            " - 2s - loss: 0.6327\n",
            "Epoch 65/100\n",
            " - 2s - loss: 0.6339\n",
            "Epoch 66/100\n",
            " - 2s - loss: 0.6330\n",
            "Epoch 67/100\n",
            " - 2s - loss: 0.6318\n",
            "Epoch 68/100\n",
            " - 2s - loss: 0.6319\n",
            "Epoch 69/100\n",
            " - 2s - loss: 0.6317\n",
            "Epoch 70/100\n",
            " - 2s - loss: 0.6312\n",
            "Epoch 71/100\n",
            " - 2s - loss: 0.6305\n",
            "Epoch 72/100\n",
            " - 2s - loss: 0.6307\n",
            "Epoch 73/100\n",
            " - 2s - loss: 0.6298\n",
            "Epoch 74/100\n",
            " - 2s - loss: 0.6302\n",
            "Epoch 75/100\n",
            " - 2s - loss: 0.6290\n",
            "Epoch 76/100\n",
            " - 2s - loss: 0.6295\n",
            "Epoch 77/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 2s - loss: 0.6288\n",
            "Epoch 78/100\n",
            " - 2s - loss: 0.6278\n",
            "Epoch 79/100\n",
            " - 2s - loss: 0.6269\n",
            "Epoch 80/100\n",
            " - 2s - loss: 0.6269\n",
            "Epoch 81/100\n",
            " - 2s - loss: 0.6262\n",
            "Epoch 82/100\n",
            " - 2s - loss: 0.6273\n",
            "Epoch 83/100\n",
            " - 2s - loss: 0.6269\n",
            "Epoch 84/100\n",
            " - 2s - loss: 0.6266\n",
            "Epoch 85/100\n",
            " - 2s - loss: 0.6253\n",
            "Epoch 86/100\n",
            " - 2s - loss: 0.6248\n",
            "Epoch 87/100\n",
            " - 2s - loss: 0.6249\n",
            "Epoch 88/100\n",
            " - 2s - loss: 0.6245\n",
            "Epoch 89/100\n",
            " - 2s - loss: 0.6253\n",
            "Epoch 90/100\n",
            " - 2s - loss: 0.6241\n",
            "Epoch 91/100\n",
            " - 2s - loss: 0.6244\n",
            "Epoch 92/100\n",
            " - 2s - loss: 0.6233\n",
            "Epoch 93/100\n",
            " - 2s - loss: 0.6239\n",
            "Epoch 94/100\n",
            " - 2s - loss: 0.6222\n",
            "Epoch 95/100\n",
            " - 2s - loss: 0.6209\n",
            "Epoch 96/100\n",
            " - 2s - loss: 0.6212\n",
            "Epoch 97/100\n",
            " - 2s - loss: 0.6217\n",
            "Epoch 98/100\n",
            " - 2s - loss: 0.6219\n",
            "Epoch 99/100\n",
            " - 2s - loss: 0.6212\n",
            "Epoch 100/100\n",
            " - 2s - loss: 0.6205\n",
            "1208/1208 [==============================] - 0s 151us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcotqO7WKZtU",
        "colab_type": "text"
      },
      "source": [
        "After the execution of the experiments, the average error is measured. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaZ0bBcqKZtV",
        "colab_type": "code",
        "colab": {},
        "outputId": "29297e31-58c7-4f08-b67b-733b55d3099c"
      },
      "source": [
        "np.mean(evaluations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.80070180671894"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HetmFFYKZtZ",
        "colab_type": "text"
      },
      "source": [
        "The results showed that the autoencoder has error of approximately 0.80, similarly to the results achieved by previous works. Therefore, the predicted ratings are considerably close to the actual ones so that the autoencoder is likely to understand the taste of the users within a small margin of error. This results indicate that a recommender system based on the proposed model will certainly present to the users films they will probably be interested to see, which is main goal of this approach. "
      ]
    }
  ]
}